# -*- coding: utf-8 -*-
"""Heart-diseas-predict-fin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Abdul-Moiz-Khan1/Heart-disease-prediction-/blob/main/Heart-diseas-predict-fin.ipynb
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
import seaborn as sns

data = pd.read_csv("heart.csv")



data.head()

# splitting in variables and targets

X = data.drop(columns = ['target'])
Y = data['target']

print(X)
print(Y)

# splitting in  train and test
X_train , X_test , Y_train , Y_test = train_test_split(X,Y , test_size = 0.2 , random_state = 42)

# now using logistic regression model
model = LogisticRegression()

# now fitting the data to training data
model.fit(X_train , Y_train)

# after this we will predict our results on testing data
Y_predict = model.predict(X_test)

accuracy = accuracy_score(Y_test, Y_predict)

print(accuracy)

print(classification_report(Y_test, Y_predict))

# confusion matrix
print("Confusion Matrix:")
conf_mat = confusion_matrix(Y_test, Y_predict)
print(conf_mat)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Disease', 'Disease'],
            yticklabels=['Not Disease', 'Disease'])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Build the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, Y_train)

# Predict on the testing data
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(Y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print(classification_report(Y_test, y_pred))

import numpy as np
# Get feature importances
feature_importances = model.feature_importances_


# Sort feature importances in descending order
indices = np.argsort(feature_importances)[::-1]

# Rearrange feature names so they match the sorted feature importances
names = [X.columns[i] for i in indices]

# Create plot
plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), feature_importances[indices])
plt.xticks(range(X.shape[1]), names, rotation=90)
plt.title("Feature Importance")
plt.show()

mypal= ['#FC05FB', '#FEAEFE', '#FCD2FC','#F3FEFA', '#B4FFE4','#3FFEBA']

plt.figure(figsize=(7, 5),facecolor='#F6F5F4')
total = float(len(data))
ax = sns.countplot(x=data['target'], palette=mypal[1::4])
ax.set_facecolor('#F6F5F4')

for p in ax.patches:

    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,height + 3,'{:1.1f} %'.format((height/total)*100), ha="center",
           bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.5))

ax.set_title('Target variable distribution', fontsize=20, y=1.05)
sns.despine(right=True)
sns.despine(offset=5, trim=True)

_ = ['age', 'sex', 'ca', 'thal', 'oldpeak', 'target']
data_ = data[_]
g = sns.pairplot(data_, hue="target", corner=True, diag_kind='hist', palette=mypal[1::4]);
plt.suptitle('Pairplot: Numerical Features ' ,fontsize = 24);

import matplotlib.pyplot as plt

# Assuming you have two lists of accuracies from two models
model1_accuracies = [0.85, 0.88, 0.88, 0.87, 0.85]
model2_accuracies = [0.82, 0.83, 0.84, 0.85, 0.81]

epochs = range(1, len(model1_accuracies) + 1)

plt.plot(epochs, model1_accuracies, 'b-', label='Model 1')  # 'b-' is for blue solid line
plt.plot(epochs, model2_accuracies, 'r-', label='Model 2')  # 'r-' is for red solid line
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy comparison between Model 1 and Model 2')
plt.legend()
plt.show()

input_data = (51,1,0,140,299,0,1,173,1,1.6,2,0,3)
inputarray = np.asarray(input_data)
input_reshape = inputarray.reshape(1,-1)
prediction = model.predict(input_reshape)
print(prediction)

